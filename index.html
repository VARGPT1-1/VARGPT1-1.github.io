<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative
    Instruction Tuning and Reinforcement Learning">
    <meta name="keywords" content="VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative
    Instruction Tuning and Reinforcement Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative
        Instruction Tuning and Reinforcement Learning</title>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://code.jquery.com/color/jquery.color-2.2.0.min.js"
        integrity="sha256-aSe2ZC5QeunlL/w/7PsVKmV+fa0eDbmybn/ptsKHR6I=" crossorigin="anonymous"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <!--    <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>-->
    <script src="./static/js/interact.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <!-- Title, Authors & Link Capsules -->
    <section class="hero section-title">
        <!--    <div class="hero-body">-->
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">
                        <span class="ours-title">
                            VARGPT-v1.1
                        </span>
                        <br />VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative
                        Instruction Tuning and Reinforcement Learning
                    </h1>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Xianwei
                                Zhuang</a><sup>1</sup>,</span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Yuxin Xie</a><sup>1</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Yufan Deng</a><sup>1</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Dongchao Yang</a><sup>2</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Liming Liang</a><sup>1</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Jinghan Ru</a><sup>1</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Yuguo Yin</a><sup>1</sup>
                        </span>
                        <span class="author-block">
                            <a href="https://vargpt1-1.github.io/" target="_blank">Yuexian Zou</a><sup>1</sup>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Peking University</span>,
                        <span class="author-block"><sup>2</sup> The Chinese University of Hong Kong</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <!-- <span class="link-block">
                                <a href="" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </span> -->
                            <span class="link-block">
                                <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv (coming soon)</span>
                                </a>
                            </span>
                            <!-- Video Link. -->
                            <span class="link-block">
                                <a href="https://youtu.be/WJXoWt90xy0" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <!-- "fas" means fa-brand -->
                                        <i class="fab fa-youtube"></i>
                                    </span>
                                    <span>Video</span>
                                </a>
                            </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                                <a href="https://github.com/VARGPT-family/VARGPT-v1.1" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>
                        </div>

                    </div>
                </div>
            </div>
            <!--        </div>-->
        </div>
    </section>

    <section class="hero teaser" id="section-interpolation">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered" style="margin-bottom:0; margin-top: 1px">
                <div class="column" id="teaser-text">
                    <p>
                        Some generated 512Ã—512 samples by VARGPT-v1.1. Our VARGPT-v1.1 supports text-and-image
                        instructions from user and
                        outputs both text-and-image mixed modal data simultaneously.
                    </p>
                </div>
            </div>
            <div class="hero-body">
                <img src="static/images/VARGPT-11-vis.jpg" alt="3D Avatar">
            </div>
        </div>
    </section>


    <!-- Abstract -->
    <section class="section" id="section-abstract">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths is-light">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that
                            builds upon our previous framework VARGPT. The model preserves the dual paradigm of
                            next-token prediction for visual understanding and next-scale generation for image
                            synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining
                            iterative visual instruction tuning with reinforcement learning through Direct Preference
                            Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative
                            instruction pairs, (3) an upgraded language backbone using Qwen2, (4) enhanced image
                            generation resolution, and (5) emergent image editing capabilities without architectural
                            modifications.
                            These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal
                            understanding and text-to-image instruction following tasks, demonstrating significant
                            improvements in both comprehension and generation metrics. Notably, through visual
                            instruction tuning, the model acquires image editing functionality while maintaining
                            architectural consistency with its predecessor, revealing the potential for unified visual
                            understanding, generation, and editing. Our findings suggest that well-designed unified
                            visual autoregressive models can effectively adopt flexible training strategies from large
                            language models (LLMs), exhibiting promising scalability.
                        </p>
                    </div>
                </div>
            </div>
    </section>
    <!--/ Abstract. -->
    <!-- Method -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3 has-text-centered">Abilities</h2>
                    <img src="static/images/comprehension.png" width="60%"
                        style="display: block; margin-left: auto; margin-right: auto;" />
                    <div class="content has-text">
                        <p>
                            A comparative analysis of various MLLMs across multi-
                            ple visual comprehension benchmarks is presented. The remaining
                            metrics are derived from standard visual question-answering bench-
                            marks and multi-modal comprehension benchmarks. Notably, our
                            VARGPT-v1.1 model demonstrates significant superiority over the
                            compared baselines across all comprehension benchmarks. </p>
                    </div>
                </div>
            </div>
    </section>
    <!-- Paper video. -->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Video</h2>
            <div class="publication-video">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/WJXoWt90xy0?si=CeFG19yAuCLCw7n-"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </div>
        </div>
    </section>

    <!-- Method -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3 has-text-centered">Model Architecture</h2>
                    <img src="static/images/VARGPT-v1-1-main_00.jpg" width="100%" />
                    <div class="content has-text">
                        <p>
                            The illustration of the proposed VARGPT-v1.1 framework similar to VARGPT, which
                            consists of (1) a LLM (Qwen2-7B-
                            Instruct), visual encoder and a understanding projector for visual understanding;
                            (2) a visual decoder and dual generation projectors
                            for visual generation. VARGPT-v1.1 employs the causal attention in the LLM backbone, while
                            utilizing block causal attention in the visual
                            decoder. </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3 has-text-centered">Training</h2>
                    <img src="static/images/VARGPT-v1-1-training_00.jpg" width="100%" />
                    <div class="content has-text">
                        <p>
                            The three training stages of the VARGPT, including stage-1 pretraining, stage-2 visual
                            instruction tuning and stage-3 iterative
                            training. </p>
                    </div>
                    <img src="static/images/VARGPT-v1-1-Stage3-Training_00.jpg" width="100%" />
                    <div class="content has-text">
                        <p>
                            The proposed iterative training strategy for the third stage gradually increases the
                            resolution of the image, while using instruction
                            fine-tuning and reinforcement learning iterative training. Finally, we use the
                            instruction-follow dataset for image editing to stimulate the
                            modelâ€™s visual editing ability. </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3 has-text-centered">Data</h2>
                    <img src="static/images/Data.jpg" width="70%"
                        style="display: block; margin-left: auto; margin-right: auto;" />
                    <div class="content has-text">
                        <p>
                            We present the data distribution we construct and collect,
                            encompassing the proportional breakdown of data across the three
                            training stages. Our composite dataset for stage-2 training is derived
                            from LLaVA-1.5, LLaVA-OneVision.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3 has-text-centered">Experiments</h2>
                    <h3>Zero-shot multi-modal evaluation</h3>
                    <img src="static/images/e-1.png" width="100%" />
                    <div class="content has-text">
                        <p>
                            Zero-shot multi-modal evaluation on multi-modal benchmarks including MMMU, MME, MMBench,
                            SEEDBench, and POPE
                            (including different settings random, popular and adversarial ). The overall scores are
                            reported for evaluation and we report test results for
                            MMBench. Gen represents whether the method supports image generation capability. VARGPT-v1.1
                            achieves the best overall performance.</p>
                    </div>
                    <h3>Performance comparison on visual question answering tasks.</h3>
                    <img src="static/images/e-2.png" width="100%" />
                    <div class="content has-text">
                        <p>
                            Performance comparison on visual question answering tasks. We gray out the model has trained
                            on the dataset. Gen represents
                            whether the method supports image generation capability.</p>
                    </div>
                    <h3>Performance comparison on visual question answering tasks.</h3>
                    <img src="static/images/e-3.png" width="100%" />
                    <h3>Performance comparison on the DPG-Bench benchmark.</h3>
                    <img src="static/images/e-4.png" width="100%" />
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3 has-text-centered">More samples</h2>
                    <h3>Visual understanding sample</h3>
                    <img src="static/images/grounding.png" width="50%"
                        style="display: block; margin-left: auto; margin-right: auto;" />

                    <h3>Generated samples</h3>
                    <img src="static/images/VARGPT-11-appendix-gen-1_00.jpg" width="100%" />
                    <img src="static/images/VARGPT-11-exp-vis_00.jpg" width="100%" />
                    <h3>Image-editing samples</h3>
                    <img src="static/images/VARGPT-11-edit_00.jpg" width="100%" />
                    <img src="static/images/VARGPT-11-appendix-edit-1_00.jpg" width="100%" />
                </div>
            </div>
        </div>
    </section>

    <!-- Related Links -->
    <section id="related-links" class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3 has-text-centered">Related Links</h2>
                        <ul>
                            <li><a href="https://vargpt-1.github.io/" target="_blank">VARGPT</a>: Unified Understanding
                                and Generation in a Visual Autoregressive Multimodal Large Language Model</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- TODO: Insert arXiv citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title" style="text-align: center">BibTeX</h2>
            <pre><code>@misc{zhuang2025vargptunifiedunderstandinggeneration,
                title={VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model}, 
                author={Xianwei Zhuang and Yuxin Xie and Yufan Deng and Liming Liang and Jinghan Ru and Yuguo Yin and Yuexian Zou},
                year={2025},
                eprint={2501.12327},
                archivePrefix={arXiv},
                primaryClass={cs.CV},
                url={https://arxiv.org/abs/2501.12327}, 
          }</code></pre>
        </div>
    </section>


</body>

<footer class="footer">
    <div class="container">
        <div class="columns">
            <div class="column is-full-width">
                <div class="content has-text-centered">
                    <p>
                        Website inspired by <a href="https://tobias-kirschstein.github.io/">Tobias Kirschstein</a>'s <a
                            href="https://tobias-kirschstein.github.io/avat3r/">avat3r website</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>

</footer>

</html>
